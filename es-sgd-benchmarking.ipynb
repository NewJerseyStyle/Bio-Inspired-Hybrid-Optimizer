{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nimport matplotlib.pyplot as plt\nimport time\nimport json\n\n# 載入並預處理MNIST數據\ndef load_data():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    return x_train, y_train, x_test, y_test\n\n# MLP類別（支援SGD和ES）\nclass MLP:\n    def __init__(self, layers):\n        self.layers = layers\n        self.weights = []\n        self.biases = []\n        for i in range(len(layers) - 1):\n            w = np.random.randn(layers[i], layers[i+1]) * 0.1\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n    \n    def relu(self, x):\n        return np.maximum(0, x)\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    \n    def forward(self, x):\n        a = x\n        for i in range(len(self.weights) - 1):\n            z = np.dot(a, self.weights[i]) + self.biases[i]\n            a = self.relu(z)\n        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n        a = self.softmax(z)\n        return a\n    \n    def get_params(self):\n        params = []\n        for w, b in zip(self.weights, self.biases):\n            params.append(w.flatten())\n            params.append(b.flatten())\n        return np.concatenate(params)\n    \n    def set_params(self, params):\n        idx = 0\n        for i in range(len(self.weights)):\n            w_shape = self.weights[i].shape\n            b_shape = self.biases[i].shape\n            w_size = np.prod(w_shape)\n            b_size = np.prod(b_shape)\n            self.weights[i] = params[idx:idx+w_size].reshape(w_shape)\n            idx += w_size\n            self.biases[i] = params[idx:idx+b_size].reshape(b_shape)\n            idx += b_size\n    \n    def evaluate(self, x, y):\n        pred = self.forward(x)\n        acc = np.mean(np.argmax(pred, axis=1) == np.argmax(y, axis=1))\n        return acc\n    \n    def compute_loss(self, x, y):\n        pred = self.forward(x)\n        pred = np.clip(pred, 1e-10, 1 - 1e-10)\n        loss = -np.mean(np.sum(y * np.log(pred), axis=1))\n        return loss\n    \n    def backward(self, x, y):\n        \"\"\"反向傳播計算梯度\"\"\"\n        m = x.shape[0]\n        activations = [x]\n        z_values = []\n        \n        # 前向傳播並保存中間值\n        a = x\n        for i in range(len(self.weights) - 1):\n            z = np.dot(a, self.weights[i]) + self.biases[i]\n            z_values.append(z)\n            a = self.relu(z)\n            activations.append(a)\n        \n        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n        z_values.append(z)\n        output = self.softmax(z)\n        \n        # 反向傳播\n        dz = output - y\n        gradients_w = []\n        gradients_b = []\n        \n        for i in range(len(self.weights) - 1, -1, -1):\n            dw = np.dot(activations[i].T, dz) / m\n            db = np.sum(dz, axis=0, keepdims=True) / m\n            gradients_w.insert(0, dw)\n            gradients_b.insert(0, db)\n            \n            if i > 0:\n                dz = np.dot(dz, self.weights[i].T)\n                dz[z_values[i-1] <= 0] = 0  # ReLU導數\n        \n        return gradients_w, gradients_b\n    \n    def sgd_update(self, x, y, learning_rate=0.01):\n        \"\"\"SGD更新（使用反向傳播）\"\"\"\n        grad_w, grad_b = self.backward(x, y)\n        for i in range(len(self.weights)):\n            self.weights[i] -= learning_rate * grad_w[i]\n            self.biases[i] -= learning_rate * grad_b[i]\n\n# 純ES優化器\nclass PureES:\n    def __init__(self, model, population_size=50, sigma=0.1, learning_rate=0.03):\n        self.model = model\n        self.pop_size = population_size\n        self.sigma = sigma\n        self.lr = learning_rate\n        self.param_size = len(model.get_params())\n    \n    def train(self, x_train, y_train, x_test, y_test, generations=200, sample_size=1000):\n        history = {\n            'train_acc': [], 'test_acc': [], 'train_loss': [],\n            'time': [], 'samples_seen': []\n        }\n        best_params = self.model.get_params()\n        total_samples = 0\n        start_time = time.time()\n        \n        for gen in range(generations):\n            idx = np.random.choice(len(x_train), sample_size, replace=False)\n            x_sample = x_train[idx]\n            y_sample = y_train[idx]\n            \n            noise = np.random.randn(self.pop_size, self.param_size)\n            rewards = np.zeros(self.pop_size)\n            \n            for i in range(self.pop_size):\n                params_try = best_params + self.sigma * noise[i]\n                self.model.set_params(params_try)\n                rewards[i] = self.model.evaluate(x_sample, y_sample)\n            \n            total_samples += self.pop_size * sample_size\n            \n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n            best_params += self.lr / (self.pop_size * self.sigma) * np.dot(noise.T, rewards)\n            self.model.set_params(best_params)\n            \n            train_acc = self.model.evaluate(x_sample, y_sample)\n            test_acc = self.model.evaluate(x_test, y_test)\n            train_loss = self.model.compute_loss(x_sample, y_sample)\n            \n            history['train_acc'].append(train_acc)\n            history['test_acc'].append(test_acc)\n            history['train_loss'].append(train_loss)\n            history['time'].append(time.time() - start_time)\n            history['samples_seen'].append(total_samples)\n            \n            if (gen + 1) % 20 == 0:\n                print(f\"Gen {gen+1}/{generations} - Train: {train_acc:.4f}, \"\n                      f\"Test: {test_acc:.4f}, Time: {history['time'][-1]:.1f}s\")\n        \n        return history\n\n# 純SGD優化器\nclass PureSGD:\n    def __init__(self, model, learning_rate=0.01, batch_size=128):\n        self.model = model\n        self.lr = learning_rate\n        self.batch_size = batch_size\n    \n    def train(self, x_train, y_train, x_test, y_test, epochs=200):\n        history = {\n            'train_acc': [], 'test_acc': [], 'train_loss': [],\n            'time': [], 'samples_seen': []\n        }\n        total_samples = 0\n        start_time = time.time()\n        \n        for epoch in range(epochs):\n            # 打亂數據\n            indices = np.random.permutation(len(x_train))\n            x_shuffled = x_train[indices]\n            y_shuffled = y_train[indices]\n            \n            num_batches = len(x_train) // self.batch_size\n            \n            for batch_idx in range(num_batches):\n                start_idx = batch_idx * self.batch_size\n                end_idx = start_idx + self.batch_size\n                x_batch = x_shuffled[start_idx:end_idx]\n                y_batch = y_shuffled[start_idx:end_idx]\n                \n                self.model.sgd_update(x_batch, y_batch, self.lr)\n                total_samples += self.batch_size\n            \n            # 評估（使用小批次以加速）\n            eval_size = 1000\n            eval_idx = np.random.choice(len(x_train), eval_size, replace=False)\n            train_acc = self.model.evaluate(x_train[eval_idx], y_train[eval_idx])\n            test_acc = self.model.evaluate(x_test, y_test)\n            train_loss = self.model.compute_loss(x_train[eval_idx], y_train[eval_idx])\n            \n            history['train_acc'].append(train_acc)\n            history['test_acc'].append(test_acc)\n            history['train_loss'].append(train_loss)\n            history['time'].append(time.time() - start_time)\n            history['samples_seen'].append(total_samples)\n            \n            if (epoch + 1) % 20 == 0:\n                print(f\"Epoch {epoch+1}/{epochs} - Train: {train_acc:.4f}, \"\n                      f\"Test: {test_acc:.4f}, Time: {history['time'][-1]:.1f}s\")\n        \n        return history\n\n# 混合ES+SGD優化器\nclass HybridESGD:\n    def __init__(self, model, pop_size=50, es_sigma=0.1, es_lr=0.03, \n                 sgd_lr=0.001, sgd_batch_size=128):\n        self.model = model\n        self.pop_size = pop_size\n        self.sigma = es_sigma\n        self.es_lr = es_lr\n        self.sgd_lr = sgd_lr\n        self.batch_size = sgd_batch_size\n        self.param_size = len(model.get_params())\n    \n    def train(self, x_train, y_train, x_test, y_test, generations=100, \n              es_sample_size=1000):\n        history = {\n            'train_acc': [], 'test_acc': [], 'train_loss': [],\n            'best_individual_acc': [], 'time': [], 'samples_seen': []\n        }\n        best_params = self.model.get_params()\n        total_samples = 0\n        start_time = time.time()\n        \n        for gen in range(generations):\n            # ES階段\n            idx = np.random.choice(len(x_train), es_sample_size, replace=False)\n            x_sample = x_train[idx]\n            y_sample = y_train[idx]\n            \n            noise = np.random.randn(self.pop_size, self.param_size)\n            rewards = np.zeros(self.pop_size)\n            all_params = []\n            \n            for i in range(self.pop_size):\n                params_try = best_params + self.sigma * noise[i]\n                all_params.append(params_try)\n                self.model.set_params(params_try)\n                rewards[i] = self.model.evaluate(x_sample, y_sample)\n            \n            total_samples += self.pop_size * es_sample_size\n            \n            best_idx = np.argmax(rewards)\n            best_individual_params = all_params[best_idx]\n            best_individual_acc = rewards[best_idx]\n            history['best_individual_acc'].append(best_individual_acc)\n            \n            # SGD階段（1 epoch）\n            self.model.set_params(best_individual_params)\n            num_batches = len(x_train) // self.batch_size\n            \n            for batch_idx in range(num_batches):\n                start_idx = batch_idx * self.batch_size\n                end_idx = start_idx + self.batch_size\n                x_batch = x_train[start_idx:end_idx]\n                y_batch = y_train[start_idx:end_idx]\n                self.model.sgd_update(x_batch, y_batch, self.sgd_lr)\n                total_samples += self.batch_size\n            \n            best_params = self.model.get_params()\n            \n            # ES更新\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n            es_update = self.es_lr / (self.pop_size * self.sigma) * np.dot(noise.T, rewards)\n            best_params += es_update\n            self.model.set_params(best_params)\n            \n            # 評估\n            eval_idx = np.random.choice(len(x_train), 1000, replace=False)\n            train_acc = self.model.evaluate(x_train[eval_idx], y_train[eval_idx])\n            test_acc = self.model.evaluate(x_test, y_test)\n            train_loss = self.model.compute_loss(x_train[eval_idx], y_train[eval_idx])\n            \n            history['train_acc'].append(train_acc)\n            history['test_acc'].append(test_acc)\n            history['train_loss'].append(train_loss)\n            history['time'].append(time.time() - start_time)\n            history['samples_seen'].append(total_samples)\n            \n            if (gen + 1) % 10 == 0:\n                print(f\"Gen {gen+1}/{generations} - Train: {train_acc:.4f}, \"\n                      f\"Test: {test_acc:.4f}, Time: {history['time'][-1]:.1f}s\")\n        \n        return history\n\n# 主實驗\nif __name__ == \"__main__\":\n    print(\"=\" * 60)\n    print(\"Evolution Strategy vs SGD Baseline Comparison\")\n    print(\"=\" * 60)\n    \n    x_train, y_train, x_test, y_test = load_data()\n    \n    results = {}\n    \n    # 實驗1: 純ES\n    print(\"\\n[1/3] 訓練純ES模型...\")\n    model_es = MLP([784, 128, 64, 10])\n    es_optimizer = PureES(model_es, population_size=50, sigma=0.1, learning_rate=0.03)\n    results['ES'] = es_optimizer.train(x_train, y_train, x_test, y_test, \n                                        generations=200, sample_size=1000)\n    \n    # 實驗2: 純SGD\n    print(\"\\n[2/3] 訓練純SGD模型...\")\n    model_sgd = MLP([784, 128, 64, 10])\n    sgd_optimizer = PureSGD(model_sgd, learning_rate=0.01, batch_size=128)\n    results['SGD'] = sgd_optimizer.train(x_train, y_train, x_test, y_test, epochs=200)\n    \n    # 實驗3: 混合ES+SGD\n    print(\"\\n[3/3] 訓練混合ES+SGD模型...\")\n    model_hybrid = MLP([784, 128, 64, 10])\n    hybrid_optimizer = HybridESGD(model_hybrid, pop_size=50, es_sigma=0.1, \n                                   es_lr=0.03, sgd_lr=0.001, sgd_batch_size=128)\n    results['Hybrid'] = hybrid_optimizer.train(x_train, y_train, x_test, y_test, \n                                                generations=100)\n    \n    # 繪製對比圖\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    \n    # 測試準確率 vs 迭代次數\n    ax = axes[0, 0]\n    for name, history in results.items():\n        iterations = range(1, len(history['test_acc']) + 1)\n        ax.plot(iterations, history['test_acc'], label=name, linewidth=2)\n    ax.set_xlabel('Iterations (Generations/Epochs)')\n    ax.set_ylabel('Test Accuracy')\n    ax.set_title('Test Accuracy vs Iterations')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 測試準確率 vs 時間\n    ax = axes[0, 1]\n    for name, history in results.items():\n        ax.plot(history['time'], history['test_acc'], label=name, linewidth=2)\n    ax.set_xlabel('Time (seconds)')\n    ax.set_ylabel('Test Accuracy')\n    ax.set_title('Test Accuracy vs Time (Efficiency)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 測試準確率 vs 樣本數\n    ax = axes[0, 2]\n    for name, history in results.items():\n        samples_millions = np.array(history['samples_seen']) / 1e6\n        ax.plot(samples_millions, history['test_acc'], label=name, linewidth=2)\n    ax.set_xlabel('Samples Seen (Millions)')\n    ax.set_ylabel('Test Accuracy')\n    ax.set_title('Test Accuracy vs Samples (Sample Efficiency)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # 訓練損失\n    ax = axes[1, 0]\n    for name, history in results.items():\n        iterations = range(1, len(history['train_loss']) + 1)\n        ax.plot(iterations, history['train_loss'], label=name, linewidth=2)\n    ax.set_xlabel('Iterations')\n    ax.set_ylabel('Training Loss')\n    ax.set_title('Training Loss vs Iterations')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_yscale('log')\n    \n    # 收斂速度比較（達到特定準確率的時間）\n    ax = axes[1, 1]\n    target_accs = [0.80, 0.85, 0.90, 0.92, 0.94]\n    x_pos = np.arange(len(target_accs))\n    width = 0.25\n    \n    for idx, (name, history) in enumerate(results.items()):\n        times_to_target = []\n        for target in target_accs:\n            test_accs = np.array(history['test_acc'])\n            times = np.array(history['time'])\n            indices = np.where(test_accs >= target)[0]\n            if len(indices) > 0:\n                times_to_target.append(times[indices[0]])\n            else:\n                times_to_target.append(times[-1])\n        ax.bar(x_pos + idx * width, times_to_target, width, label=name)\n    \n    ax.set_xlabel('Target Accuracy')\n    ax.set_ylabel('Time to Reach (seconds)')\n    ax.set_title('Convergence Speed Comparison')\n    ax.set_xticks(x_pos + width)\n    ax.set_xticklabels([f'{acc:.0%}' for acc in target_accs])\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    # 最終性能比較\n    ax = axes[1, 2]\n    methods = list(results.keys())\n    final_test_acc = [results[m]['test_acc'][-1] for m in methods]\n    final_time = [results[m]['time'][-1] for m in methods]\n    final_samples = [results[m]['samples_seen'][-1] / 1e6 for m in methods]\n    \n    x_pos = np.arange(len(methods))\n    width = 0.25\n    ax.bar(x_pos - width, final_test_acc, width, label='Test Accuracy', alpha=0.8)\n    ax.bar(x_pos, [t/max(final_time) for t in final_time], width, \n           label='Rel. Time', alpha=0.8)\n    ax.bar(x_pos + width, [s/max(final_samples) for s in final_samples], width,\n           label='Rel. Samples', alpha=0.8)\n    \n    ax.set_ylabel('Value (normalized)')\n    ax.set_title('Final Performance Metrics')\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(methods)\n    ax.legend()\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('es_sgd_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # 打印統計摘要\n    print(\"\\n\" + \"=\" * 60)\n    print(\"FINAL RESULTS SUMMARY\")\n    print(\"=\" * 60)\n    for name, history in results.items():\n        print(f\"\\n{name}:\")\n        print(f\"  最終測試準確率: {history['test_acc'][-1]:.4f}\")\n        print(f\"  最佳測試準確率: {max(history['test_acc']):.4f}\")\n        print(f\"  總訓練時間: {history['time'][-1]:.1f}秒\")\n        print(f\"  總樣本數: {history['samples_seen'][-1]/1e6:.2f}M\")\n        print(f\"  樣本效率 (acc/M samples): {history['test_acc'][-1]/(history['samples_seen'][-1]/1e6):.4f}\")\n        print(f\"  時間效率 (acc/min): {history['test_acc'][-1]/(history['time'][-1]/60):.4f}\")\n    \n    # 保存結果到JSON\n    summary = {\n        name: {\n            'final_test_acc': float(history['test_acc'][-1]),\n            'best_test_acc': float(max(history['test_acc'])),\n            'total_time': float(history['time'][-1]),\n            'total_samples': int(history['samples_seen'][-1]),\n            'iterations': len(history['test_acc'])\n        }\n        for name, history in results.items()\n    }\n    \n    with open('comparison_results.json', 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    print(\"\\n結果已保存至: es_sgd_comparison.png 和 comparison_results.json\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nimport matplotlib.pyplot as plt\nimport time\nimport json\n\n# 載入並預處理MNIST數據\ndef load_data():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    return x_train, y_train, x_test, y_test\n\n# Learning Rate Schedulers\nclass LinearWarmupCosineAnnealing:\n    \"\"\"Linear warmup followed by cosine annealing\"\"\"\n    def __init__(self, max_lr, min_lr, warmup_steps, total_steps):\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n    \n    def get_lr(self, step):\n        if step < self.warmup_steps:\n            # Linear warmup\n            return self.min_lr + (self.max_lr - self.min_lr) * step / self.warmup_steps\n        else:\n            # Cosine annealing\n            progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n            return self.min_lr + (self.max_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n\nclass AdaptiveSigmaScheduler:\n    \"\"\"Adaptive sigma for ES based on improvement rate\"\"\"\n    def __init__(self, initial_sigma, min_sigma, max_sigma, decay_factor=0.99):\n        self.sigma = initial_sigma\n        self.min_sigma = min_sigma\n        self.max_sigma = max_sigma\n        self.decay_factor = decay_factor\n        self.prev_best_reward = None\n    \n    def update(self, best_reward, avg_reward):\n        \"\"\"Update sigma based on progress\"\"\"\n        if self.prev_best_reward is not None:\n            improvement = best_reward - self.prev_best_reward\n            \n            if improvement > 0.01:  # Good progress, can decay sigma\n                self.sigma *= self.decay_factor\n            elif improvement < 0.001:  # Stagnation, increase exploration\n                self.sigma *= 1.02\n        \n        self.prev_best_reward = best_reward\n        self.sigma = np.clip(self.sigma, self.min_sigma, self.max_sigma)\n        return self.sigma\n\n# MLP類別\nclass MLP:\n    def __init__(self, layers):\n        self.layers = layers\n        self.weights = []\n        self.biases = []\n        for i in range(len(layers) - 1):\n            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n    \n    def relu(self, x):\n        return np.maximum(0, x)\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    \n    def forward(self, x):\n        a = x\n        for i in range(len(self.weights) - 1):\n            z = np.dot(a, self.weights[i]) + self.biases[i]\n            a = self.relu(z)\n        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n        a = self.softmax(z)\n        return a\n    \n    def get_params(self):\n        params = []\n        for w, b in zip(self.weights, self.biases):\n            params.append(w.flatten())\n            params.append(b.flatten())\n        return np.concatenate(params)\n    \n    def set_params(self, params):\n        idx = 0\n        for i in range(len(self.weights)):\n            w_shape = self.weights[i].shape\n            b_shape = self.biases[i].shape\n            w_size = np.prod(w_shape)\n            b_size = np.prod(b_shape)\n            self.weights[i] = params[idx:idx+w_size].reshape(w_shape)\n            idx += w_size\n            self.biases[i] = params[idx:idx+b_size].reshape(b_shape)\n            idx += b_size\n    \n    def evaluate(self, x, y):\n        pred = self.forward(x)\n        acc = np.mean(np.argmax(pred, axis=1) == np.argmax(y, axis=1))\n        return acc\n    \n    def compute_loss(self, x, y):\n        pred = self.forward(x)\n        pred = np.clip(pred, 1e-10, 1 - 1e-10)\n        loss = -np.mean(np.sum(y * np.log(pred), axis=1))\n        return loss\n    \n    def backward(self, x, y):\n        m = x.shape[0]\n        activations = [x]\n        z_values = []\n        \n        a = x\n        for i in range(len(self.weights) - 1):\n            z = np.dot(a, self.weights[i]) + self.biases[i]\n            z_values.append(z)\n            a = self.relu(z)\n            activations.append(a)\n        \n        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n        z_values.append(z)\n        output = self.softmax(z)\n        \n        dz = output - y\n        gradients_w = []\n        gradients_b = []\n        \n        for i in range(len(self.weights) - 1, -1, -1):\n            dw = np.dot(activations[i].T, dz) / m\n            db = np.sum(dz, axis=0, keepdims=True) / m\n            gradients_w.insert(0, dw)\n            gradients_b.insert(0, db)\n            \n            if i > 0:\n                dz = np.dot(dz, self.weights[i].T)\n                dz[z_values[i-1] <= 0] = 0\n        \n        return gradients_w, gradients_b\n    \n    def sgd_update(self, x, y, learning_rate=0.01):\n        grad_w, grad_b = self.backward(x, y)\n        for i in range(len(self.weights)):\n            self.weights[i] -= learning_rate * grad_w[i]\n            self.biases[i] -= learning_rate * grad_b[i]\n\n# 純ES優化器（帶adaptive sigma）\nclass PureESAdaptive:\n    def __init__(self, model, population_size=50, initial_sigma=0.1, learning_rate=0.03):\n        self.model = model\n        self.pop_size = population_size\n        self.lr = learning_rate\n        self.param_size = len(model.get_params())\n        self.sigma_scheduler = AdaptiveSigmaScheduler(\n            initial_sigma=initial_sigma,\n            min_sigma=0.001,\n            max_sigma=0.3,\n            decay_factor=0.995\n        )\n    \n    def train(self, x_train, y_train, x_test, y_test, generations=200, sample_size=1000):\n        history = {\n            'train_acc': [], 'test_acc': [], 'train_loss': [],\n            'time': [], 'samples_seen': [], 'sigma': []\n        }\n        best_params = self.model.get_params()\n        total_samples = 0\n        start_time = time.time()\n        \n        for gen in range(generations):\n            idx = np.random.choice(len(x_train), sample_size, replace=False)\n            x_sample = x_train[idx]\n            y_sample = y_train[idx]\n            \n            sigma = self.sigma_scheduler.sigma\n            noise = np.random.randn(self.pop_size, self.param_size)\n            rewards = np.zeros(self.pop_size)\n            \n            for i in range(self.pop_size):\n                params_try = best_params + sigma * noise[i]\n                self.model.set_params(params_try)\n                rewards[i] = self.model.evaluate(x_sample, y_sample)\n            \n            total_samples += self.pop_size * sample_size\n            \n            # Update sigma based on progress\n            best_reward = np.max(rewards)\n            avg_reward = np.mean(rewards)\n            sigma = self.sigma_scheduler.update(best_reward, avg_reward)\n            \n            rewards_norm = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n            best_params += self.lr / (self.pop_size * sigma) * np.dot(noise.T, rewards_norm)\n            self.model.set_params(best_params)\n            \n            train_acc = self.model.evaluate(x_sample, y_sample)\n            test_acc = self.model.evaluate(x_test, y_test)\n            train_loss = self.model.compute_loss(x_sample, y_sample)\n            \n            history['train_acc'].append(train_acc)\n            history['test_acc'].append(test_acc)\n            history['train_loss'].append(train_loss)\n            history['time'].append(time.time() - start_time)\n            history['samples_seen'].append(total_samples)\n            history['sigma'].append(sigma)\n            \n            if (gen + 1) % 20 == 0:\n                print(f\"Gen {gen+1}/{generations} - Train: {train_acc:.4f}, \"\n                      f\"Test: {test_acc:.4f}, Sigma: {sigma:.4f}, Time: {history['time'][-1]:.1f}s\")\n        \n        return history\n\n# SGD with Linear Warmup + Cosine Annealing\nclass SGDWithSchedule:\n    def __init__(self, model, max_lr=0.1, min_lr=0.0001, batch_size=128, warmup_epochs=10):\n        self.model = model\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.batch_size = batch_size\n        self.warmup_epochs = warmup_epochs\n    \n    def train(self, x_train, y_train, x_test, y_test, epochs=200):\n        history = {\n            'train_acc': [], 'test_acc': [], 'train_loss': [],\n            'time': [], 'samples_seen': [], 'lr': []\n        }\n        total_samples = 0\n        start_time = time.time()\n        \n        num_batches_per_epoch = len(x_train) // self.batch_size\n        total_steps = epochs * num_batches_per_epoch\n        warmup_steps = self.warmup_epochs * num_batches_per_epoch\n        \n        lr_scheduler = LinearWarmupCosineAnnealing(\n            max_lr=self.max_lr,\n            min_lr=self.min_lr,\n            warmup_steps=warmup_steps,\n            total_steps=total_steps\n        )\n        \n        global_step = 0\n        \n        for epoch in range(epochs):\n            indices = np.random.permutation(len(x_train))\n            x_shuffled = x_train[indices]\n            y_shuffled = y_train[indices]\n            \n            for batch_idx in range(num_batches_per_epoch):\n                start_idx = batch_idx * self.batch_size\n                end_idx = start_idx + self.batch_size\n                x_batch = x_shuffled[start_idx:end_idx]\n                y_batch = y_shuffled[start_idx:end_idx]\n                \n                lr = lr_scheduler.get_lr(global_step)\n                self.model.sgd_update(x_batch, y_batch, lr)\n                \n                total_samples += self.batch_size\n                global_step += 1\n            \n            eval_size = 1000\n            eval_idx = np.random.choice(len(x_train), eval_size, replace=False)\n            train_acc = self.model.evaluate(x_train[eval_idx], y_train[eval_idx])\n            test_acc = self.model.evaluate(x_test, y_test)\n            train_loss = self.model.compute_loss(x_train[eval_idx], y_train[eval_idx])\n            current_lr = lr_scheduler.get_lr(global_step - 1)\n            \n            history['train_acc'].append(train_acc)\n            history['test_acc'].append(test_acc)\n            history['train_loss'].append(train_loss)\n            history['time'].append(time.time() - start_time)\n            history['samples_seen'].append(total_samples)\n            history['lr'].append(current_lr)\n            \n            if (epoch + 1) % 20 == 0:\n                print(f\"Epoch {epoch+1}/{epochs} - Train: {train_acc:.4f}, \"\n                      f\"Test: {test_acc:.4f}, LR: {current_lr:.6f}, Time: {history['time'][-1]:.1f}s\")\n        \n        return history\n\n# Hybrid ES+SGD with adaptive schedules (Sequential Strategy)\nclass HybridESGDAdaptive:\n    \"\"\"\n    Hybrid strategy: ES for exploration -> SGD for exploitation\n    Key: ES and SGD are applied SEQUENTIALLY, not overlapping\n    \"\"\"\n    def __init__(self, model, pop_size=50, initial_es_sigma=0.1, es_lr=0.03,\n                 max_sgd_lr=0.01, min_sgd_lr=0.0001, sgd_batch_size=128):\n        self.model = model\n        self.pop_size = pop_size\n        self.es_lr = es_lr\n        self.batch_size = sgd_batch_size\n        self.param_size = len(model.get_params())\n        \n        # Adaptive sigma for ES\n        self.sigma_scheduler = AdaptiveSigmaScheduler(\n            initial_sigma=initial_es_sigma,\n            min_sigma=0.001,\n            max_sigma=0.2,\n            decay_factor=0.995\n        )\n        \n        # Store SGD LR range\n        self.max_sgd_lr = max_sgd_lr\n        self.min_sgd_lr = min_sgd_lr\n    \n    def train(self, x_train, y_train, x_test, y_test, generations=100, \n              es_sample_size=1000):\n        history = {\n            'train_acc': [], 'test_acc': [], 'train_loss': [],\n            'best_individual_acc': [], 'time': [], 'samples_seen': [],\n            'sigma': [], 'sgd_lr': []\n        }\n        best_params = self.model.get_params()\n        total_samples = 0\n        start_time = time.time()\n        \n        # SGD scheduler for the hybrid approach\n        num_batches_per_gen = len(x_train) // self.batch_size\n        total_sgd_steps = generations * num_batches_per_gen\n        \n        sgd_scheduler = LinearWarmupCosineAnnealing(\n            max_lr=self.max_sgd_lr,\n            min_lr=self.min_sgd_lr,\n            warmup_steps=10 * num_batches_per_gen,  # 10 generations warmup\n            total_steps=total_sgd_steps\n        )\n        \n        global_sgd_step = 0\n        \n        for gen in range(generations):\n            # ========== PHASE 1: ES Exploration ==========\n            idx = np.random.choice(len(x_train), es_sample_size, replace=False)\n            x_sample = x_train[idx]\n            y_sample = y_train[idx]\n            \n            sigma = self.sigma_scheduler.sigma\n            noise = np.random.randn(self.pop_size, self.param_size)\n            rewards = np.zeros(self.pop_size)\n            all_params = []\n            \n            # Evaluate population\n            for i in range(self.pop_size):\n                params_try = best_params + sigma * noise[i]\n                all_params.append(params_try)\n                self.model.set_params(params_try)\n                rewards[i] = self.model.evaluate(x_sample, y_sample)\n            \n            total_samples += self.pop_size * es_sample_size\n            \n            # Find best individual from ES exploration\n            best_idx = np.argmax(rewards)\n            best_individual_params = all_params[best_idx]\n            best_individual_acc = rewards[best_idx]\n            history['best_individual_acc'].append(best_individual_acc)\n            \n            # Update sigma based on ES performance\n            avg_reward = np.mean(rewards)\n            sigma = self.sigma_scheduler.update(best_individual_acc, avg_reward)\n            \n            # ========== PHASE 2: SGD Local Refinement ==========\n            # Start from the best ES individual\n            self.model.set_params(best_individual_params)\n            \n            # Run SGD for one epoch on the best individual\n            indices = np.random.permutation(len(x_train))\n            x_shuffled = x_train[indices]\n            y_shuffled = y_train[indices]\n            \n            for batch_idx in range(num_batches_per_gen):\n                start_idx = batch_idx * self.batch_size\n                end_idx = start_idx + self.batch_size\n                x_batch = x_shuffled[start_idx:end_idx]\n                y_batch = y_shuffled[start_idx:end_idx]\n                \n                current_sgd_lr = sgd_scheduler.get_lr(global_sgd_step)\n                self.model.sgd_update(x_batch, y_batch, current_sgd_lr)\n                \n                total_samples += self.batch_size\n                global_sgd_step += 1\n            \n            # SGD-refined parameters become the new baseline for next generation\n            best_params = self.model.get_params()\n            \n            # Evaluate final model (after both ES and SGD)\n            eval_idx = np.random.choice(len(x_train), 1000, replace=False)\n            train_acc = self.model.evaluate(x_train[eval_idx], y_train[eval_idx])\n            test_acc = self.model.evaluate(x_test, y_test)\n            train_loss = self.model.compute_loss(x_train[eval_idx], y_train[eval_idx])\n            \n            history['train_acc'].append(train_acc)\n            history['test_acc'].append(test_acc)\n            history['train_loss'].append(train_loss)\n            history['time'].append(time.time() - start_time)\n            history['samples_seen'].append(total_samples)\n            history['sigma'].append(sigma)\n            history['sgd_lr'].append(current_sgd_lr)\n            \n            if (gen + 1) % 10 == 0:\n                print(f\"Gen {gen+1}/{generations} - Best ES: {best_individual_acc:.4f}, \"\n                      f\"After SGD: {train_acc:.4f}, Test: {test_acc:.4f}, \"\n                      f\"Sigma: {sigma:.4f}, SGD_LR: {current_sgd_lr:.6f}, \"\n                      f\"Time: {history['time'][-1]:.1f}s\")\n        \n        return history\n\n# 主實驗\nif __name__ == \"__main__\":\n    print(\"=\" * 70)\n    print(\"Evolution Strategy vs SGD - Advanced Comparison with Adaptive Schedules\")\n    print(\"=\" * 70)\n    \n    x_train, y_train, x_test, y_test = load_data()\n    results = {}\n    \n    # 實驗1: ES with Adaptive Sigma\n    print(\"\\n[1/3] Training ES with Adaptive Sigma...\")\n    model_es = MLP([784, 128, 64, 10])\n    es_optimizer = PureESAdaptive(model_es, population_size=50, \n                                   initial_sigma=0.1, learning_rate=0.03)\n    results['ES-Adaptive'] = es_optimizer.train(x_train, y_train, x_test, y_test, \n                                                 generations=200, sample_size=1000)\n    \n    # 實驗2: SGD with Linear Warmup + Cosine Annealing\n    print(\"\\n[2/3] Training SGD with Warmup+Cosine Schedule...\")\n    model_sgd = MLP([784, 128, 64, 10])\n    sgd_optimizer = SGDWithSchedule(model_sgd, max_lr=0.1, min_lr=0.0001, \n                                     batch_size=128, warmup_epochs=10)\n    results['SGD-Scheduled'] = sgd_optimizer.train(x_train, y_train, x_test, y_test, \n                                                    epochs=200)\n    \n    # 實驗3: Hybrid ES+SGD with Adaptive Schedules\n    print(\"\\n[3/3] Training Hybrid ES+SGD with Adaptive Schedules...\")\n    model_hybrid = MLP([784, 128, 64, 10])\n    hybrid_optimizer = HybridESGDAdaptive(model_hybrid, pop_size=50, \n                                           initial_es_sigma=0.1, es_lr=0.03,\n                                           max_sgd_lr=0.01, min_sgd_lr=0.0001,\n                                           sgd_batch_size=128)\n    results['Hybrid-Adaptive'] = hybrid_optimizer.train(x_train, y_train, x_test, y_test, \n                                                         generations=100)\n    \n    # 繪製對比圖\n    fig = plt.figure(figsize=(20, 12))\n    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n    \n    # Row 1: Performance Metrics\n    ax1 = fig.add_subplot(gs[0, 0])\n    for name, history in results.items():\n        iterations = range(1, len(history['test_acc']) + 1)\n        ax1.plot(iterations, history['test_acc'], label=name, linewidth=2)\n    ax1.set_xlabel('Iterations')\n    ax1.set_ylabel('Test Accuracy')\n    ax1.set_title('Test Accuracy vs Iterations')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    ax2 = fig.add_subplot(gs[0, 1])\n    for name, history in results.items():\n        ax2.plot(history['time'], history['test_acc'], label=name, linewidth=2)\n    ax2.set_xlabel('Time (seconds)')\n    ax2.set_ylabel('Test Accuracy')\n    ax2.set_title('Test Accuracy vs Time')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    ax3 = fig.add_subplot(gs[0, 2])\n    for name, history in results.items():\n        samples_millions = np.array(history['samples_seen']) / 1e6\n        ax3.plot(samples_millions, history['test_acc'], label=name, linewidth=2)\n    ax3.set_xlabel('Samples Seen (Millions)')\n    ax3.set_ylabel('Test Accuracy')\n    ax3.set_title('Sample Efficiency')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    ax4 = fig.add_subplot(gs[0, 3])\n    for name, history in results.items():\n        iterations = range(1, len(history['train_loss']) + 1)\n        ax4.plot(iterations, history['train_loss'], label=name, linewidth=2)\n    ax4.set_xlabel('Iterations')\n    ax4.set_ylabel('Training Loss')\n    ax4.set_title('Training Loss')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    ax4.set_yscale('log')\n    \n    # Row 2: Adaptive Parameters\n    ax5 = fig.add_subplot(gs[1, 0])\n    if 'sigma' in results['ES-Adaptive']:\n        iterations = range(1, len(results['ES-Adaptive']['sigma']) + 1)\n        ax5.plot(iterations, results['ES-Adaptive']['sigma'], \n                label='ES-Adaptive', color='C0', linewidth=2)\n    if 'sigma' in results['Hybrid-Adaptive']:\n        iterations = range(1, len(results['Hybrid-Adaptive']['sigma']) + 1)\n        ax5.plot(iterations, results['Hybrid-Adaptive']['sigma'], \n                label='Hybrid-Adaptive', color='C2', linewidth=2)\n    ax5.set_xlabel('Iterations')\n    ax5.set_ylabel('Sigma (Noise Level)')\n    ax5.set_title('ES Sigma Adaptation')\n    ax5.legend()\n    ax5.grid(True, alpha=0.3)\n    \n    ax6 = fig.add_subplot(gs[1, 1])\n    if 'lr' in results['SGD-Scheduled']:\n        iterations = range(1, len(results['SGD-Scheduled']['lr']) + 1)\n        ax6.plot(iterations, results['SGD-Scheduled']['lr'], \n                label='SGD-Scheduled', color='C1', linewidth=2)\n    if 'sgd_lr' in results['Hybrid-Adaptive']:\n        iterations = range(1, len(results['Hybrid-Adaptive']['sgd_lr']) + 1)\n        ax6.plot(iterations, results['Hybrid-Adaptive']['sgd_lr'], \n                label='Hybrid-Adaptive', color='C2', linewidth=2)\n    ax6.set_xlabel('Iterations')\n    ax6.set_ylabel('Learning Rate')\n    ax6.set_title('SGD Learning Rate Schedule')\n    ax6.legend()\n    ax6.grid(True, alpha=0.3)\n    ax6.set_yscale('log')\n    \n    # Convergence speed\n    ax7 = fig.add_subplot(gs[1, 2])\n    target_accs = [0.80, 0.85, 0.90, 0.92, 0.94]\n    x_pos = np.arange(len(target_accs))\n    width = 0.25\n    \n    for idx, (name, history) in enumerate(results.items()):\n        times_to_target = []\n        for target in target_accs:\n            test_accs = np.array(history['test_acc'])\n            times = np.array(history['time'])\n            indices = np.where(test_accs >= target)[0]\n            if len(indices) > 0:\n                times_to_target.append(times[indices[0]])\n            else:\n                times_to_target.append(times[-1])\n        ax7.bar(x_pos + idx * width, times_to_target, width, label=name)\n    \n    ax7.set_xlabel('Target Accuracy')\n    ax7.set_ylabel('Time to Reach (seconds)')\n    ax7.set_title('Convergence Speed')\n    ax7.set_xticks(x_pos + width)\n    ax7.set_xticklabels([f'{acc:.0%}' for acc in target_accs])\n    ax7.legend()\n    ax7.grid(True, alpha=0.3, axis='y')\n    \n    # Final performance comparison\n    ax8 = fig.add_subplot(gs[1, 3])\n    methods = list(results.keys())\n    final_test_acc = [results[m]['test_acc'][-1] for m in methods]\n    final_time = [results[m]['time'][-1] for m in methods]\n    final_samples = [results[m]['samples_seen'][-1] / 1e6 for m in methods]\n    \n    x_pos = np.arange(len(methods))\n    width = 0.25\n    ax8.bar(x_pos - width, final_test_acc, width, label='Test Acc', alpha=0.8)\n    ax8.bar(x_pos, [t/max(final_time) for t in final_time], width, \n           label='Rel. Time', alpha=0.8)\n    ax8.bar(x_pos + width, [s/max(final_samples) for s in final_samples], width,\n           label='Rel. Samples', alpha=0.8)\n    \n    ax8.set_ylabel('Value')\n    ax8.set_title('Final Performance')\n    ax8.set_xticks(x_pos)\n    ax8.set_xticklabels(methods, rotation=15, ha='right')\n    ax8.legend()\n    ax8.grid(True, alpha=0.3, axis='y')\n    \n    # Row 3: Training dynamics\n    ax9 = fig.add_subplot(gs[2, :2])\n    for name, history in results.items():\n        iterations = range(1, len(history['test_acc']) + 1)\n        ax9.plot(iterations, history['test_acc'], label=f'{name} (Test)', linewidth=2)\n        ax9.plot(iterations, history['train_acc'], label=f'{name} (Train)', \n                linewidth=1.5, linestyle='--', alpha=0.7)\n    ax9.set_xlabel('Iterations')\n    ax9.set_ylabel('Accuracy')\n    ax9.set_title('Train vs Test Accuracy')\n    ax9.legend(ncol=2, fontsize=8)\n    ax9.grid(True, alpha=0.3)\n    \n    # Efficiency metrics\n    ax10 = fig.add_subplot(gs[2, 2:])\n    metrics_names = ['Final Acc', 'Time Efficiency\\n(acc/min)', 'Sample Efficiency\\n(acc/M samples)']\n    \n    for idx, name in enumerate(methods):\n        history = results[name]\n        final_acc = history['test_acc'][-1]\n        time_eff = final_acc / (history['time'][-1] / 60)\n        sample_eff = final_acc / (history['samples_seen'][-1] / 1e6)\n        \n        values = [final_acc, time_eff * 10, sample_eff]  # Scale for visibility\n        x_pos = np.arange(len(metrics_names))\n        ax10.bar(x_pos + idx * 0.25, values, 0.25, label=name, alpha=0.8)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nimport matplotlib.pyplot as plt\nimport time\nimport json\n\n# 載入MNIST數據\ndef load_data():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    return x_train, y_train, x_test, y_test\n\n# MLP基礎類別\nclass MLP:\n    def __init__(self, layers):\n        self.layers = layers\n        self.weights = []\n        self.biases = []\n        for i in range(len(layers) - 1):\n            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n    \n    def relu(self, x):\n        return np.maximum(0, x)\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    \n    def forward(self, x):\n        a = x\n        for i in range(len(self.weights) - 1):\n            z = np.dot(a, self.weights[i]) + self.biases[i]\n            a = self.relu(z)\n        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n        a = self.softmax(z)\n        return a\n    \n    def get_params(self):\n        params = []\n        for w, b in zip(self.weights, self.biases):\n            params.append(w.flatten())\n            params.append(b.flatten())\n        return np.concatenate(params)\n    \n    def set_params(self, params):\n        idx = 0\n        for i in range(len(self.weights)):\n            w_shape = self.weights[i].shape\n            b_shape = self.biases[i].shape\n            w_size = np.prod(w_shape)\n            b_size = np.prod(b_shape)\n            self.weights[i] = params[idx:idx+w_size].reshape(w_shape)\n            idx += w_size\n            self.biases[i] = params[idx:idx+b_size].reshape(b_shape)\n            idx += b_size\n    \n    def evaluate(self, x, y):\n        pred = self.forward(x)\n        acc = np.mean(np.argmax(pred, axis=1) == np.argmax(y, axis=1))\n        return acc\n    \n    def compute_loss(self, x, y):\n        pred = self.forward(x)\n        pred = np.clip(pred, 1e-10, 1 - 1e-10)\n        loss = -np.mean(np.sum(y * np.log(pred), axis=1))\n        return loss\n    \n    def backward(self, x, y):\n        m = x.shape[0]\n        activations = [x]\n        z_values = []\n        \n        a = x\n        for i in range(len(self.weights) - 1):\n            z = np.dot(a, self.weights[i]) + self.biases[i]\n            z_values.append(z)\n            a = self.relu(z)\n            activations.append(a)\n        \n        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n        z_values.append(z)\n        output = self.softmax(z)\n        \n        dz = output - y\n        gradients_w = []\n        gradients_b = []\n        \n        for i in range(len(self.weights) - 1, -1, -1):\n            dw = np.dot(activations[i].T, dz) / m\n            db = np.sum(dz, axis=0, keepdims=True) / m\n            gradients_w.insert(0, dw)\n            gradients_b.insert(0, db)\n            \n            if i > 0:\n                dz = np.dot(dz, self.weights[i].T)\n                dz[z_values[i-1] <= 0] = 0\n        \n        return gradients_w, gradients_b\n    \n    def sgd_update(self, x, y, learning_rate=0.01):\n        grad_w, grad_b = self.backward(x, y)\n        for i in range(len(self.weights)):\n            self.weights[i] -= learning_rate * grad_w[i]\n            self.biases[i] -= learning_rate * grad_b[i]\n\n# Advanced Hybrid ES-SGD based on OpenAI-ES insights\nclass AdvancedHybridOptimizer:\n    \"\"\"\n    基於OpenAI-ES論文的進階混合優化器\n    \n    關鍵特性：\n    1. Limited Perturbations: 只擾動部分參數（大幅加速）\n    2. Rank-based Fitness Shaping: 使用排名而非原始獎勵\n    3. Interleaved ES-SGD: 交錯使用ES探索和SGD精煉\n    4. Adaptive Noise & LR: 動態調整探索範圍和學習率\n    \"\"\"\n    \n    def __init__(self, model, \n                 pop_size=50,\n                 initial_sigma=0.1,\n                 es_lr=0.03,\n                 max_sgd_lr=0.01,\n                 min_sgd_lr=0.0001,\n                 sgd_batch_size=128,\n                 perturb_fraction=0.5):  # 擾動參數的比例\n        self.model = model\n        self.pop_size = pop_size\n        self.es_lr = es_lr\n        self.batch_size = sgd_batch_size\n        self.param_size = len(model.get_params())\n        \n        # Limited Perturbations: 只擾動部分參數\n        self.perturb_size = int(self.param_size * perturb_fraction)\n        print(f\"Limited Perturbations: {self.perturb_size}/{self.param_size} \"\n              f\"({perturb_fraction*100:.0f}%) parameters will be perturbed\")\n        \n        # Adaptive schedulers\n        self.sigma = initial_sigma\n        self.min_sigma = 0.001\n        self.max_sigma = 0.3\n        self.sigma_decay = 0.995\n        \n        self.max_sgd_lr = max_sgd_lr\n        self.min_sgd_lr = min_sgd_lr\n        \n        # Track performance for adaptive adjustment\n        self.prev_best_fitness = None\n        self.stagnation_count = 0\n    \n    def rank_fitness_shaping(self, rewards):\n        \"\"\"Rank-based fitness shaping (更穩定的獎勵信號)\"\"\"\n        ranks = np.argsort(np.argsort(rewards))  # 獲得排名\n        # 使用utility函數轉換排名\n        utilities = np.maximum(0, np.log(self.pop_size/2 + 1) - np.log(ranks + 1))\n        utilities = utilities / np.sum(utilities) - 1.0 / self.pop_size\n        return utilities\n    \n    def limited_perturbation_noise(self):\n        \"\"\"\n        Limited Perturbations: 只擾動部分參數\n        基於OpenAI-ES論文，可獲得60+倍加速\n        \"\"\"\n        # 為每個個體隨機選擇要擾動的參數索引\n        perturbations = []\n        indices_list = []\n        \n        for _ in range(self.pop_size):\n            # 隨機選擇要擾動的參數索引\n            indices = np.random.choice(self.param_size, self.perturb_size, replace=False)\n            indices_list.append(indices)\n            \n            # 創建稀疏擾動向量\n            noise = np.zeros(self.param_size)\n            noise[indices] = np.random.randn(self.perturb_size)\n            perturbations.append(noise)\n        \n        return np.array(perturbations), indices_list\n    \n    def update_sigma(self, best_fitness):\n        \"\"\"基於性能動態調整sigma\"\"\"\n        if self.prev_best_fitness is not None:\n            improvement = best_fitness - self.prev_best_fitness\n            \n            if improvement > 0.01:\n                # 顯著進步：減小探索範圍\n                self.sigma *= self.sigma_decay\n                self.stagnation_count = 0\n            elif improvement < 0.001:\n                # 停滯：增加探索\n                self.stagnation_count += 1\n                if self.stagnation_count > 5:\n                    self.sigma *= 1.05\n                    self.stagnation_count = 0\n        \n        self.prev_best_fitness = best_fitness\n        self.sigma = np.clip(self.sigma, self.min_sigma, self.max_sigma)\n    \n    def train(self, x_train, y_train, x_test, y_test, \n              generations=100,\n              es_sample_size=1000,\n              sgd_frequency=1):  # 每N代做一次SGD\n        \"\"\"\n        交錯式ES-SGD訓練\n        sgd_frequency: 控制ES和SGD的交錯頻率\n        \"\"\"\n        history = {\n            'train_acc': [], 'test_acc': [], 'train_loss': [],\n            'best_individual_acc': [], 'time': [], 'samples_seen': [],\n            'sigma': [], 'sgd_lr': [], 'es_gradient_norm': []\n        }\n        \n        best_params = self.model.get_params()\n        total_samples = 0\n        start_time = time.time()\n        \n        # SGD scheduler\n        num_batches_per_gen = len(x_train) // self.batch_size\n        total_sgd_steps = (generations // sgd_frequency) * num_batches_per_gen\n        global_sgd_step = 0\n        \n        for gen in range(generations):\n            # ========== ES Phase ==========\n            idx = np.random.choice(len(x_train), es_sample_size, replace=False)\n            x_sample = x_train[idx]\n            y_sample = y_train[idx]\n            \n            # Limited Perturbations\n            noise, indices_list = self.limited_perturbation_noise()\n            rewards = np.zeros(self.pop_size)\n            all_params = []\n            \n            # 評估擾動族群\n            for i in range(self.pop_size):\n                params_try = best_params + self.sigma * noise[i]\n                all_params.append(params_try)\n                self.model.set_params(params_try)\n                rewards[i] = self.model.evaluate(x_sample, y_sample)\n            \n            total_samples += self.pop_size * es_sample_size\n            \n            # Rank-based fitness shaping\n            utilities = self.rank_fitness_shaping(rewards)\n            \n            # 計算ES梯度估計\n            es_gradient = np.dot(noise.T, utilities) / self.sigma\n            es_gradient_norm = np.linalg.norm(es_gradient)\n            \n            # 更新參數\n            best_params += self.es_lr * es_gradient\n            \n            # 找到最佳個體\n            best_idx = np.argmax(rewards)\n            best_individual_acc = rewards[best_idx]\n            \n            # 更新sigma\n            self.update_sigma(best_individual_acc)\n            \n            # ========== SGD Phase (交錯執行) ==========\n            if (gen + 1) % sgd_frequency == 0:\n                # 從ES優化後的參數開始SGD\n                self.model.set_params(best_params)\n                \n                # 計算當前SGD學習率\n                progress = global_sgd_step / max(total_sgd_steps, 1)\n                current_sgd_lr = self.min_sgd_lr + (self.max_sgd_lr - self.min_sgd_lr) * \\\n                                 0.5 * (1 + np.cos(np.pi * progress))\n                \n                # Mini-batch SGD\n                indices = np.random.permutation(len(x_train))\n                x_shuffled = x_train[indices]\n                y_shuffled = y_train[indices]\n                \n                for batch_idx in range(num_batches_per_gen):\n                    start_idx = batch_idx * self.batch_size\n                    end_idx = start_idx + self.batch_size\n                    x_batch = x_shuffled[start_idx:end_idx]\n                    y_batch = y_shuffled[start_idx:end_idx]\n                    \n                    self.model.sgd_update(x_batch, y_batch, current_sgd_lr)\n                    total_samples += self.batch_size\n                    global_sgd_step += 1\n                \n                # SGD後的參數成為新基準\n                best_params = self.model.get_params()\n            else:\n                current_sgd_lr = 0  # 這一代沒有SGD\n            \n            # 評估\n            self.model.set_params(best_params)\n            eval_idx = np.random.choice(len(x_train), 1000, replace=False)\n            train_acc = self.model.evaluate(x_train[eval_idx], y_train[eval_idx])\n            test_acc = self.model.evaluate(x_test, y_test)\n            train_loss = self.model.compute_loss(x_train[eval_idx], y_train[eval_idx])\n            \n            history['train_acc'].append(train_acc)\n            history['test_acc'].append(test_acc)\n            history['train_loss'].append(train_loss)\n            history['best_individual_acc'].append(best_individual_acc)\n            history['time'].append(time.time() - start_time)\n            history['samples_seen'].append(total_samples)\n            history['sigma'].append(self.sigma)\n            history['sgd_lr'].append(current_sgd_lr)\n            history['es_gradient_norm'].append(es_gradient_norm)\n            \n            if (gen + 1) % 10 == 0:\n                sgd_status = f\"SGD_LR: {current_sgd_lr:.6f}\" if current_sgd_lr > 0 else \"No SGD\"\n                print(f\"Gen {gen+1}/{generations} - Best ES: {best_individual_acc:.4f}, \"\n                      f\"Test: {test_acc:.4f}, Sigma: {self.sigma:.4f}, {sgd_status}, \"\n                      f\"ES_grad_norm: {es_gradient_norm:.2f}, Time: {history['time'][-1]:.1f}s\")\n        \n        return history\n\n# 主實驗\nif __name__ == \"__main__\":\n    print(\"=\" * 70)\n    print(\"Advanced Hybrid ES-SGD Optimizer (Based on OpenAI-ES Insights)\")\n    print(\"=\" * 70)\n    \n    x_train, y_train, x_test, y_test = load_data()\n    \n    # 測試不同的配置\n    configs = {\n        'Limited-Pert-50%': {\n            'perturb_fraction': 0.5,\n            'sgd_frequency': 1,\n            'description': 'Limited Perturbations (50% params) + SGD每代'\n        },\n        'Limited-Pert-25%': {\n            'perturb_fraction': 0.25,\n            'sgd_frequency': 1,\n            'description': 'Limited Perturbations (25% params) + SGD每代'\n        },\n        'Interleaved-ES-SGD': {\n            'perturb_fraction': 0.5,\n            'sgd_frequency': 3,\n            'description': 'Limited Pert (50%) + SGD每3代'\n        },\n        'Pure-ES-Limited': {\n            'perturb_fraction': 0.5,\n            'sgd_frequency': 999,  # 實質上不做SGD\n            'description': 'Pure ES with Limited Perturbations'\n        }\n    }\n    \n    results = {}\n    \n    for name, config in configs.items():\n        print(f\"\\n{'='*70}\")\n        print(f\"Training: {name}\")\n        print(f\"Config: {config['description']}\")\n        print(f\"{'='*70}\")\n        \n        model = MLP([784, 128, 64, 10])\n        optimizer = AdvancedHybridOptimizer(\n            model=model,\n            pop_size=50,\n            initial_sigma=0.1,\n            es_lr=0.03,\n            max_sgd_lr=0.01,\n            min_sgd_lr=0.0001,\n            sgd_batch_size=128,\n            perturb_fraction=config['perturb_fraction']\n        )\n        \n        results[name] = optimizer.train(\n            x_train, y_train, x_test, y_test,\n            generations=100,\n            es_sample_size=1000,\n            sgd_frequency=config['sgd_frequency']\n        )\n    \n    # 可視化比較\n    fig = plt.figure(figsize=(20, 12))\n    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n    \n    # Test Accuracy\n    ax1 = fig.add_subplot(gs[0, 0])\n    for name, history in results.items():\n        ax1.plot(history['test_acc'], label=name, linewidth=2)\n    ax1.set_xlabel('Generation')\n    ax1.set_ylabel('Test Accuracy')\n    ax1.set_title('Test Accuracy Comparison')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Time Efficiency\n    ax2 = fig.add_subplot(gs[0, 1])\n    for name, history in results.items():\n        ax2.plot(history['time'], history['test_acc'], label=name, linewidth=2)\n    ax2.set_xlabel('Time (seconds)')\n    ax2.set_ylabel('Test Accuracy')\n    ax2.set_title('Time Efficiency')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Sample Efficiency\n    ax3 = fig.add_subplot(gs[0, 2])\n    for name, history in results.items():\n        samples_m = np.array(history['samples_seen']) / 1e6\n        ax3.plot(samples_m, history['test_acc'], label=name, linewidth=2)\n    ax3.set_xlabel('Samples (Millions)')\n    ax3.set_ylabel('Test Accuracy')\n    ax3.set_title('Sample Efficiency')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Sigma Evolution\n    ax4 = fig.add_subplot(gs[1, 0])\n    for name, history in results.items():\n        ax4.plot(history['sigma'], label=name, linewidth=2)\n    ax4.set_xlabel('Generation')\n    ax4.set_ylabel('Sigma')\n    ax4.set_title('Adaptive Sigma Evolution')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    # ES Gradient Norm\n    ax5 = fig.add_subplot(gs[1, 1])\n    for name, history in results.items():\n        ax5.plot(history['es_gradient_norm'], label=name, linewidth=2, alpha=0.7)\n    ax5.set_xlabel('Generation')\n    ax5.set_ylabel('ES Gradient Norm')\n    ax5.set_title('ES Gradient Magnitude')\n    ax5.legend()\n    ax5.grid(True, alpha=0.3)\n    ax5.set_yscale('log')\n    \n    # Training Loss\n    ax6 = fig.add_subplot(gs[1, 2])\n    for name, history in results.items():\n        ax6.plot(history['train_loss'], label=name, linewidth=2)\n    ax6.set_xlabel('Generation')\n    ax6.set_ylabel('Training Loss')\n    ax6.set_title('Training Loss')\n    ax6.legend()\n    ax6.grid(True, alpha=0.3)\n    ax6.set_yscale('log')\n    \n    # Best Individual vs Final\n    ax7 = fig.add_subplot(gs[2, 0])\n    for name, history in results.items():\n        ax7.plot(history['best_individual_acc'], label=f'{name} (ES)', \n                linestyle='--', alpha=0.7)\n        ax7.plot(history['test_acc'], label=f'{name} (Final)', linewidth=2)\n    ax7.set_xlabel('Generation')\n    ax7.set_ylabel('Accuracy')\n    ax7.set_title('ES Best vs Final Accuracy')\n    ax7.legend(fontsize=8)\n    ax7.grid(True, alpha=0.3)\n    \n    # Final Performance Bar Chart\n    ax8 = fig.add_subplot(gs[2, 1:])\n    methods = list(results.keys())\n    final_acc = [results[m]['test_acc'][-1] for m in methods]\n    final_time = [results[m]['time'][-1] for m in methods]\n    final_samples = [results[m]['samples_seen'][-1] / 1e6 for m in methods]\n    \n    x = np.arange(len(methods))\n    width = 0.25\n    \n    ax8.bar(x - width, final_acc, width, label='Final Accuracy', alpha=0.8)\n    ax8.bar(x, [t/max(final_time) for t in final_time], width, \n           label='Relative Time', alpha=0.8)\n    ax8.bar(x + width, [s/max(final_samples) for s in final_samples], width,\n           label='Relative Samples', alpha=0.8)\n    \n    ax8.set_ylabel('Value')\n    ax8.set_title('Final Performance Metrics')\n    ax8.set_xticks(x)\n    ax8.set_xticklabels(methods, rotation=20, ha='right')\n    ax8.legend()\n    ax8.grid(True, alpha=0.3, axis='y')\n    \n    plt.savefig('advanced_hybrid_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # 打印結果摘要\n    print(\"\\n\" + \"=\" * 70)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\" * 70)\n    for name, history in results.items():\n        print(f\"\\n{name}:\")\n        print(f\"  Final Test Acc: {history['test_acc'][-1]:.4f}\")\n        print(f\"  Best Test Acc: {max(history['test_acc']):.4f}\")\n        print(f\"  Total Time: {history['time'][-1]:.1f}s\")\n        print(f\"  Total Samples: {history['samples_seen'][-1]/1e6:.2f}M\")\n        print(f\"  Sample Efficiency: {history['test_acc'][-1]/(history['samples_seen'][-1]/1e6):.4f} acc/M\")\n        print(f\"  Time Efficiency: {history['test_acc'][-1]/(history['time'][-1]/60):.4f} acc/min\")\n        print(f\"  Final Sigma: {history['sigma'][-1]:.6f}\")\n    \n    # 保存結果\n    summary = {\n        name: {\n            'config': configs[name]['description'],\n            'final_test_acc': float(history['test_acc'][-1]),\n            'best_test_acc': float(max(history['test_acc'])),\n            'total_time': float(history['time'][-1]),\n            'total_samples': int(history['samples_seen'][-1])\n        }\n        for name, history in results.items()\n    }\n    \n    with open('advanced_hybrid_results.json', 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    print(\"\\nResults saved: advanced_hybrid_comparison.png and advanced_hybrid_results.json\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nimport matplotlib.pyplot as plt\nimport time\n\ndef load_data():\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n    y_train = keras.utils.to_categorical(y_train, 10)\n    y_test = keras.utils.to_categorical(y_test, 10)\n    return x_train, y_train, x_test, y_test\n\nclass MLP:\n    def __init__(self, layers):\n        self.layers = layers\n        self.weights = []\n        self.biases = []\n        for i in range(len(layers) - 1):\n            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n    \n    def relu(self, x):\n        return np.maximum(0, x)\n    \n    def softmax(self, x):\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    \n    def forward(self, x):\n        a = x\n        for i in range(len(self.weights) - 1):\n            z = np.dot(a, self.weights[i]) + self.biases[i]\n            a = self.relu(z)\n        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n        return self.softmax(z)\n    \n    def get_params(self):\n        params = []\n        for w, b in zip(self.weights, self.biases):\n            params.append(w.flatten())\n            params.append(b.flatten())\n        return np.concatenate(params)\n    \n    def set_params(self, params):\n        idx = 0\n        for i in range(len(self.weights)):\n            w_shape, b_shape = self.weights[i].shape, self.biases[i].shape\n            w_size, b_size = np.prod(w_shape), np.prod(b_shape)\n            self.weights[i] = params[idx:idx+w_size].reshape(w_shape)\n            idx += w_size\n            self.biases[i] = params[idx:idx+b_size].reshape(b_shape)\n            idx += b_size\n    \n    def evaluate(self, x, y):\n        pred = self.forward(x)\n        return np.mean(np.argmax(pred, axis=1) == np.argmax(y, axis=1))\n    \n    def compute_loss(self, x, y):\n        pred = self.forward(x)\n        pred = np.clip(pred, 1e-10, 1 - 1e-10)\n        return -np.mean(np.sum(y * np.log(pred), axis=1))\n    \n    def backward(self, x, y):\n        m = x.shape[0]\n        activations = [x]\n        z_values = []\n        \n        a = x\n        for i in range(len(self.weights) - 1):\n            z = np.dot(a, self.weights[i]) + self.biases[i]\n            z_values.append(z)\n            a = self.relu(z)\n            activations.append(a)\n        \n        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n        z_values.append(z)\n        output = self.softmax(z)\n        \n        dz = output - y\n        gradients_w, gradients_b = [], []\n        \n        for i in range(len(self.weights) - 1, -1, -1):\n            dw = np.dot(activations[i].T, dz) / m\n            db = np.sum(dz, axis=0, keepdims=True) / m\n            gradients_w.insert(0, dw)\n            gradients_b.insert(0, db)\n            \n            if i > 0:\n                dz = np.dot(dz, self.weights[i].T)\n                dz[z_values[i-1] <= 0] = 0\n        \n        return gradients_w, gradients_b\n    \n    def sgd_update(self, x, y, learning_rate=0.01):\n        grad_w, grad_b = self.backward(x, y)\n        for i in range(len(self.weights)):\n            self.weights[i] -= learning_rate * grad_w[i]\n            self.biases[i] -= learning_rate * grad_b[i]\n    \n    def copy(self):\n        \"\"\"Create a deep copy of the model\"\"\"\n        new_model = MLP(self.layers)\n        new_model.set_params(self.get_params().copy())\n        return new_model\n\nclass BioInspiredHybridOptimizer:\n    \"\"\"\n    生物啟發的混合優化器：\n    \n    模擬自然演化：\n    1. 每一代（Generation）生成多個個體（多尺度變異）\n    2. 每個個體在其\"生命週期\"內學習（SGD優化）\n    3. 選擇最佳個體的\"基因\"（參數）傳遞給下一代\n    \n    多尺度變異（Multi-Scale Mutation）：\n    - 同一代中包含不同幅度的變異（小、中、大）\n    - 自適應調整變異範圍\n    \"\"\"\n    \n    def __init__(self, model,\n                 population_size=15,  # 較小族群，每個個體都會被優化\n                 num_sigma_levels=3,   # 多尺度變異層級\n                 initial_sigma=0.05,   # 初始變異範圍（較小）\n                 max_sgd_lr=0.01,\n                 min_sgd_lr=0.001,\n                 sgd_steps_per_life=200,  # 每個個體的\"生命長度\"\n                 batch_size=128):\n        \n        self.base_model = model\n        self.pop_size = population_size\n        self.num_sigma_levels = num_sigma_levels\n        self.sigma = initial_sigma\n        self.max_sgd_lr = max_sgd_lr\n        self.min_sgd_lr = min_sgd_lr\n        self.sgd_steps = sgd_steps_per_life\n        self.batch_size = batch_size\n        self.param_size = len(model.get_params())\n        \n        # Evolution path for CMA-like adaptation\n        self.evolution_path = np.zeros(self.param_size)\n        self.path_decay = 0.8\n        \n        print(f\"\\n生物啟發混合優化器配置:\")\n        print(f\"  族群大小: {self.pop_size}\")\n        print(f\"  變異層級: {self.num_sigma_levels} (小/中/大突變)\")\n        print(f\"  每個體生命週期: {self.sgd_steps} SGD步驟\")\n        print(f\"  初始變異範圍: {self.sigma}\")\n    \n    def generate_multi_scale_population(self, parent_params):\n        \"\"\"\n        生成多尺度變異的族群\n        每一代包含不同範圍的突變，模擬自然界的多樣性\n        \"\"\"\n        population = []\n        sigma_used = []\n        \n        # 計算不同尺度的sigma\n        sigma_levels = [\n            self.sigma * 0.3,   # 小突變（微調）\n            self.sigma * 1.0,   # 中等突變（探索）\n            self.sigma * 3.0    # 大突變（跳出局部最優）\n        ]\n        \n        individuals_per_level = self.pop_size // self.num_sigma_levels\n        \n        for level_idx, sigma_level in enumerate(sigma_levels):\n            for _ in range(individuals_per_level):\n                # 生成擾動\n                noise = np.random.randn(self.param_size)\n                offspring_params = parent_params + sigma_level * noise\n                \n                population.append(offspring_params)\n                sigma_used.append(sigma_level)\n        \n        # 填充剩餘個體（如果pop_size不能被level數整除）\n        while len(population) < self.pop_size:\n            noise = np.random.randn(self.param_size)\n            offspring_params = parent_params + self.sigma * noise\n            population.append(offspring_params)\n            sigma_used.append(self.sigma)\n        \n        return population, sigma_used\n    \n    def live_and_learn(self, individual_params, x_train, y_train, gen_progress):\n        \"\"\"\n        個體的\"生命週期\"：在其基因基礎上通過經驗學習（SGD）\n        \n        類比：\n        - individual_params: 遺傳的基因\n        - SGD: 個體一生的學習和適應\n        - 返回值: 個體達到的最佳狀態\n        \"\"\"\n        # 創建個體模型\n        individual = self.base_model.copy()\n        individual.set_params(individual_params)\n        \n        # 學習率衰減（模擬從年輕到年老的學習能力）\n        lr_schedule = lambda step: self.min_sgd_lr + \\\n            (self.max_sgd_lr - self.min_sgd_lr) * (1 - step / self.sgd_steps)\n        \n        # 在生命週期內學習\n        best_fitness = 0\n        best_params = individual_params.copy()\n        \n        for step in range(self.sgd_steps):\n            # 隨機採樣訓練數據\n            idx = np.random.choice(len(x_train), self.batch_size, replace=False)\n            x_batch = x_train[idx]\n            y_batch = y_train[idx]\n            \n            # 學習（SGD更新）\n            current_lr = lr_schedule(step)\n            individual.sgd_update(x_batch, y_batch, current_lr)\n            \n            # 定期評估（不是每步都評估，節省時間）\n            if step % 50 == 0 or step == self.sgd_steps - 1:\n                fitness = individual.evaluate(x_batch, y_batch)\n                if fitness > best_fitness:\n                    best_fitness = fitness\n                    best_params = individual.get_params().copy()\n        \n        return best_params, best_fitness\n    \n    def update_evolution_path(self, selected_direction):\n        \"\"\"更新演化路徑（類似CMA-ES）\"\"\"\n        self.evolution_path = self.path_decay * self.evolution_path + \\\n                              (1 - self.path_decay) * selected_direction\n    \n    def adapt_sigma(self, fitness_improvements):\n        \"\"\"\n        根據族群的適應度改進情況調整變異範圍\n        如果改進良好→減小sigma（利用）\n        如果停滯→增大sigma（探索）\n        \"\"\"\n        avg_improvement = np.mean(fitness_improvements)\n        \n        if avg_improvement > 0.02:  # 顯著進步\n            self.sigma *= 0.95\n        elif avg_improvement < 0.005:  # 停滯\n            self.sigma *= 1.05\n        \n        # 限制範圍\n        self.sigma = np.clip(self.sigma, 0.001, 0.3)\n    \n    def train(self, x_train, y_train, x_test, y_test, generations=50):\n        \"\"\"\n        主訓練循環：模擬多代演化\n        \"\"\"\n        history = {\n            'train_acc': [], 'test_acc': [], 'train_loss': [],\n            'best_individual_acc': [], 'time': [], 'samples_seen': [],\n            'sigma': [], 'population_diversity': []\n        }\n        \n        # 初始\"祖先\"\n        best_parent_params = self.base_model.get_params()\n        prev_best_fitness = 0\n        \n        total_samples = 0\n        start_time = time.time()\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"開始演化訓練 ({generations} 代)\")\n        print(f\"{'='*70}\\n\")\n        \n        for gen in range(generations):\n            gen_start = time.time()\n            gen_progress = gen / generations\n            \n            # 1. 生成多尺度變異的後代\n            population, sigma_used = self.generate_multi_scale_population(best_parent_params)\n            \n            # 2. 每個個體\"活著並學習\"\n            print(f\"Generation {gen+1}/{generations} - 個體學習中...\", end='', flush=True)\n            \n            fitness_list = []\n            learned_population = []\n            initial_fitness = []\n            \n            for idx, individual_params in enumerate(population):\n                # 記錄初始適應度（遺傳的基因）\n                temp_model = self.base_model.copy()\n                temp_model.set_params(individual_params)\n                eval_idx = np.random.choice(len(x_train), 500, replace=False)\n                init_fit = temp_model.evaluate(x_train[eval_idx], y_train[eval_idx])\n                initial_fitness.append(init_fit)\n                \n                # 個體學習\n                learned_params, final_fitness = self.live_and_learn(\n                    individual_params, x_train, y_train, gen_progress\n                )\n                \n                learned_population.append(learned_params)\n                fitness_list.append(final_fitness)\n                total_samples += self.sgd_steps * self.batch_size\n                \n                if (idx + 1) % 5 == 0:\n                    print(f\".\", end='', flush=True)\n            \n            print(\" 完成\")\n            \n            # 3. 選擇（自然選擇：最適者生存）\n            best_idx = np.argmax(fitness_list)\n            best_offspring_params = learned_population[best_idx]\n            best_fitness = fitness_list[best_idx]\n            best_individual_acc = fitness_list[best_idx]\n            \n            # 計算學習帶來的提升\n            fitness_improvements = [fitness_list[i] - initial_fitness[i] \n                                   for i in range(len(fitness_list))]\n            avg_improvement = np.mean(fitness_improvements)\n            \n            # 4. 更新演化路徑\n            direction = best_offspring_params - best_parent_params\n            self.update_evolution_path(direction)\n            \n            # 5. 自適應調整sigma\n            self.adapt_sigma(fitness_improvements)\n            \n            # 6. 最佳個體成為下一代的\"父母\"\n            best_parent_params = best_offspring_params\n            self.base_model.set_params(best_parent_params)\n            \n            # 評估\n            eval_idx = np.random.choice(len(x_train), 1000, replace=False)\n            train_acc = self.base_model.evaluate(x_train[eval_idx], y_train[eval_idx])\n            test_acc = self.base_model.evaluate(x_test, y_test)\n            train_loss = self.base_model.compute_loss(x_train[eval_idx], y_train[eval_idx])\n            \n            # 計算族群多樣性\n            population_std = np.std([np.linalg.norm(p - best_parent_params) \n                                    for p in learned_population])\n            \n            # 記錄\n            history['train_acc'].append(train_acc)\n            history['test_acc'].append(test_acc)\n            history['train_loss'].append(train_loss)\n            history['best_individual_acc'].append(best_individual_acc)\n            history['time'].append(time.time() - start_time)\n            history['samples_seen'].append(total_samples)\n            history['sigma'].append(self.sigma)\n            history['population_diversity'].append(population_std)\n            \n            gen_time = time.time() - gen_start\n            \n            print(f\"  最佳個體: {best_individual_acc:.4f} | \"\n                  f\"測試準確率: {test_acc:.4f} | \"\n                  f\"平均學習提升: {avg_improvement:.4f} | \"\n                  f\"Sigma: {self.sigma:.4f} | \"\n                  f\"代時間: {gen_time:.1f}s\\n\")\n            \n            prev_best_fitness = best_fitness\n        \n        return history\n\nif __name__ == \"__main__\":\n    print(\"=\"*70)\n    print(\"生物啟發混合優化器：多尺度演化 + 個體學習\")\n    print(\"=\"*70)\n    \n    x_train, y_train, x_test, y_test = load_data()\n    \n    # 測試不同配置\n    configs = {\n        'Bio-Hybrid-Fast': {\n            'pop_size': 10,\n            'sgd_steps': 150,\n            'generations': 50,\n            'desc': '小族群，快速學習'\n        },\n        'Bio-Hybrid-Balanced': {\n            'pop_size': 15,\n            'sgd_steps': 200,\n            'generations': 40,\n            'desc': '平衡配置'\n        },\n        'Bio-Hybrid-Deep': {\n            'pop_size': 20,\n            'sgd_steps': 250,\n            'generations': 30,\n            'desc': '大族群，深度學習'\n        }\n    }\n    \n    results = {}\n    \n    for name, config in configs.items():\n        print(f\"\\n{'='*70}\")\n        print(f\"測試配置: {name}\")\n        print(f\"說明: {config['desc']}\")\n        print(f\"{'='*70}\")\n        \n        model = MLP([784, 128, 64, 10])\n        optimizer = BioInspiredHybridOptimizer(\n            model=model,\n            population_size=config['pop_size'],\n            num_sigma_levels=3,\n            initial_sigma=0.05,\n            max_sgd_lr=0.01,\n            min_sgd_lr=0.001,\n            sgd_steps_per_life=config['sgd_steps'],\n            batch_size=128\n        )\n        \n        results[name] = optimizer.train(\n            x_train, y_train, x_test, y_test,\n            generations=config['generations']\n        )\n    \n    # 可視化\n    fig = plt.figure(figsize=(20, 12))\n    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n    \n    # Test Accuracy\n    ax1 = fig.add_subplot(gs[0, 0])\n    for name, history in results.items():\n        ax1.plot(history['test_acc'], label=name, linewidth=2, marker='o', markersize=3)\n    ax1.set_xlabel('Generation')\n    ax1.set_ylabel('Test Accuracy')\n    ax1.set_title('Test Accuracy vs Generation')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Time Efficiency\n    ax2 = fig.add_subplot(gs[0, 1])\n    for name, history in results.items():\n        ax2.plot(history['time'], history['test_acc'], label=name, linewidth=2)\n    ax2.set_xlabel('Time (seconds)')\n    ax2.set_ylabel('Test Accuracy')\n    ax2.set_title('Learning Curve (Time)')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Sample Efficiency\n    ax3 = fig.add_subplot(gs[0, 2])\n    for name, history in results.items():\n        samples_m = np.array(history['samples_seen']) / 1e6\n        ax3.plot(samples_m, history['test_acc'], label=name, linewidth=2)\n    ax3.set_xlabel('Samples Seen (Millions)')\n    ax3.set_ylabel('Test Accuracy')\n    ax3.set_title('Sample Efficiency')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    # Sigma Evolution (Multi-scale adaptation)\n    ax4 = fig.add_subplot(gs[1, 0])\n    for name, history in results.items():\n        ax4.plot(history['sigma'], label=name, linewidth=2)\n    ax4.set_xlabel('Generation')\n    ax4.set_ylabel('Sigma (Mutation Range)')\n    ax4.set_title('Adaptive Mutation Range')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    ax4.set_yscale('log')\n    \n    # Population Diversity\n    ax5 = fig.add_subplot(gs[1, 1])\n    for name, history in results.items():\n        ax5.plot(history['population_diversity'], label=name, linewidth=2, alpha=0.7)\n    ax5.set_xlabel('Generation')\n    ax5.set_ylabel('Population Diversity (Std of Params)')\n    ax5.set_title('Population Diversity Over Time')\n    ax5.legend()\n    ax5.grid(True, alpha=0.3)\n    \n    # Training Loss\n    ax6 = fig.add_subplot(gs[1, 2])\n    for name, history in results.items():\n        ax6.plot(history['train_loss'], label=name, linewidth=2)\n    ax6.set_xlabel('Generation')\n    ax6.set_ylabel('Training Loss')\n    ax6.set_title('Training Loss')\n    ax6.legend()\n    ax6.grid(True, alpha=0.3)\n    ax6.set_yscale('log')\n    \n    # Learning Impact (Best Individual vs Final)\n    ax7 = fig.add_subplot(gs[2, 0])\n    for name, history in results.items():\n        improvements = [history['test_acc'][i] - history['best_individual_acc'][i] \n                       for i in range(len(history['test_acc']))]\n        ax7.plot(improvements, label=name, linewidth=2, alpha=0.7)\n    ax7.set_xlabel('Generation')\n    ax7.set_ylabel('Learning Improvement')\n    ax7.set_title('Within-Life Learning Impact')\n    ax7.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n    ax7.legend()\n    ax7.grid(True, alpha=0.3)\n    \n    # Convergence Comparison\n    ax8 = fig.add_subplot(gs[2, 1:])\n    methods = list(results.keys())\n    final_acc = [results[m]['test_acc'][-1] for m in methods]\n    best_acc = [max(results[m]['test_acc']) for m in methods]\n    final_time = [results[m]['time'][-1] for m in methods]\n    total_samples = [results[m]['samples_seen'][-1] / 1e6 for m in methods]\n    \n    x = np.arange(len(methods))\n    width = 0.2\n    \n    ax8.bar(x - 1.5*width, final_acc, width, label='Final Test Acc', alpha=0.8)\n    ax8.bar(x - 0.5*width, best_acc, width, label='Best Test Acc', alpha=0.8)\n    ax8.bar(x + 0.5*width, [t/max(final_time) for t in final_time], width,\n           label='Relative Time', alpha=0.8)\n    ax8.bar(x + 1.5*width, [s/max(total_samples) for s in total_samples], width,\n           label='Relative Samples', alpha=0.8)\n    \n    ax8.set_ylabel('Value')\n    ax8.set_title('Final Performance Comparison')\n    ax8.set_xticks(x)\n    ax8.set_xticklabels(methods, rotation=15, ha='right')\n    ax8.legend()\n    ax8.grid(True, alpha=0.3, axis='y')\n    \n    plt.savefig('bio_inspired_hybrid.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # 結果摘要\n    print(\"\\n\" + \"=\"*70)\n    print(\"最終結果摘要\")\n    print(\"=\"*70)\n    for name, history in results.items():\n        print(f\"\\n{name}:\")\n        print(f\"  最終測試準確率: {history['test_acc'][-1]:.4f}\")\n        print(f\"  最佳測試準確率: {max(history['test_acc']):.4f}\")\n        print(f\"  總訓練時間: {history['time'][-1]:.1f}秒\")\n        print(f\"  總樣本數: {history['samples_seen'][-1]/1e6:.2f}M\")\n        print(f\"  最終Sigma: {history['sigma'][-1]:.6f}\")\n        print(f\"  樣本效率: {history['test_acc'][-1]/(history['samples_seen'][-1]/1e6):.4f} acc/M\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}